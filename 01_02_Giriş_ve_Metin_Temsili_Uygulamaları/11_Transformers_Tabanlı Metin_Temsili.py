# -*- coding: utf-8 -*-
"""
Created on Sat May 17 19:59:59 2025

@author: Zekeriya
"""

# ==============================================
# ğŸ§  Transformers TabanlÄ± Metin Temsili (BERT)
# Model: bert-base-uncased (Hugging Face Transformers)
# ==============================================

# ğŸ“¦ Gerekli KÃ¼tÃ¼phaneler
from transformers import AutoTokenizer, AutoModel
import torch

# ==============================================
# ğŸ“¥ Model ve Tokenizer YÃ¼kleme

# "bert-base-uncased" adlÄ± Ã¶nceden eÄŸitilmiÅŸ modeli kullanÄ±yoruz.
# Bu model Ä°ngilizce cÃ¼mleleri kÃ¼Ã§Ã¼k harfe Ã§evirerek iÅŸler (uncased).
model_name = "bert-base-uncased"

# Tokenizer: Metni BERTâ€™in anlayacaÄŸÄ± token formatÄ±na Ã§evirir
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Model: BERT modelini yÃ¼kler (transformer encoder bloklarÄ± iÃ§erir)
model = AutoModel.from_pretrained(model_name)

# ==============================================
# ğŸ“ Metin GiriÅŸi
text = "Transformers can be used for natural language processing."

# Tokenizasyon iÅŸlemi
# return_tensors="pt" â†’ sonucu PyTorch tensÃ¶rÃ¼ (tensor) olarak dÃ¶ndÃ¼rÃ¼r
inputs = tokenizer(text, return_tensors="pt")

# ==============================================
# âš™ï¸ Model ile GÃ¶sterim (Embedding) Elde Etme

# torch.no_grad(): EÄŸitim deÄŸil, sadece Ã§Ä±karÄ±m (inference) yapacaÄŸÄ±mÄ±z iÃ§in
# bellekte yer tutacak gradyan hesaplamasÄ±nÄ± kapatÄ±r.
# gradyanlarÄ±n hesaplamasÄ± durdurulur
# bÃ¶ylece belleÄŸi daha verimli kullanÄ±rÄ±z
with torch.no_grad():
    outputs = model(**inputs)

# outputs.last_hidden_state: TÃ¼m tokenlar iÃ§in son katmandaki vektÃ¶r Ã§Ä±ktÄ±sÄ±
# Åekil: [batch_size, sequence_length, hidden_size]
last_hidden_state = outputs.last_hidden_state

# Ä°lk token'in (CLS token) embeddingâ€™ini al
first_token_embedding = last_hidden_state[0, 0, :].numpy()

# Sonucu yazdÄ±r
print("ğŸ”¹ Metin Temsili (CLS token embedding):")
print(first_token_embedding)
