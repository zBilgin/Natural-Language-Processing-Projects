# =============================================
# ğŸ§  Word Embedding GÃ¶rselleÅŸtirme - Word2Vec & FastText
# GeliÅŸtirici: Google (Word2Vec), Meta (FastText)
# =============================================

# ğŸ“¦ Gerekli KÃ¼tÃ¼phaneler
import pandas as pd
import matplotlib.pyplot as plt

# PCA: Principal Component Analysis
# Verideki boyutlarÄ± azaltarak, Ã¶nemli bilgileri koruyarak
# yeni bir Ã¶zellik (bileÅŸen) seti oluÅŸturur.
# BÃ¶ylece yÃ¼ksek boyutlu veriler gÃ¶rselleÅŸtirilebilir hale gelir.
from sklearn.decomposition import PCA  # Boyut indirgeme (dimension reduction)

from gensim.models import Word2Vec, FastText
from gensim.utils import simple_preprocess


# =============================================
# ğŸ§¾ Ã–rnek CÃ¼mle Listesi
sentences = [
    "KÃ¶pek Ã§ok tatlÄ± bir hayvandÄ±r.",
    "KÃ¶pekler evcil hayvanlardÄ±r.",
    "Kediler genellikle baÄŸÄ±msÄ±z hareket etmeyi severler.",
    "KÃ¶pekler sadÄ±k ve dost canlÄ±sÄ± hayvanlardÄ±r.",
    "Hayvanlar insanlar iÃ§in iyi arkadaÅŸlardÄ±r."
]

# ğŸ§¼ Ã–n Temizleme (Basit tokenizasyon iÅŸlemi)
# Gensim'in simple_preprocess fonksiyonu kÃ¼Ã§Ã¼k harfe Ã§evirme,
# noktalama temizliÄŸi vb. iÅŸlemleri yapar.
tokenized_sentences = [simple_preprocess(sentence) for sentence in sentences]

# =============================================
# ğŸ§  Word2Vec Modeli EÄŸitimi
word2vec_model = Word2Vec(
    sentences=tokenized_sentences,   # EÄŸitim verisi
    vector_size=50,                  # Kelime gÃ¶mme (embedding) boyutu
    window=5,                        # BaÄŸlam penceresi (Ã¶n ve arkadaki 5 kelime)
    min_count=1,                     # Minimum geÃ§me sayÄ±sÄ± (1 kere geÃ§en kelimeler dahil)
    sg=0                             # CBOW (sg=0), sg=1 yaparsan Skip-Gram olur
)

# =============================================
# âš¡ FastText Modeli EÄŸitimi
fasttext_model = FastText(
    sentences=tokenized_sentences,
    vector_size=50,
    window=5,
    min_count=1,
    sg=0
)

# =============================================
# ğŸ“Š PCA ile Word Embedding GÃ¶rselleÅŸtirme Fonksiyonu

def plot_word_embedding(model, title):
    word_vectors = model.wv

    # Normalde kelime sayÄ±sÄ± Ã§ok olabilir, ilk 1000 taneyle sÄ±nÄ±rla
    words = list(word_vectors.index_to_key)[:1000]
    vectors = [word_vectors[word] for word in words]

    # PCA: 50 boyutlu vektÃ¶rÃ¼ 3 boyuta indir
    pca = PCA(n_components=3)
    reduced_vectors = pca.fit_transform(vectors)

    # 3D GÃ¶rselleÅŸtirme
    fig = plt.figure(figsize=(10, 8))
    ax = fig.add_subplot(111, projection="3d")
    ax.scatter(reduced_vectors[:, 0], reduced_vectors[:, 1], reduced_vectors[:, 2])

    # Kelimeleri grafikte etiketle
    for i, word in enumerate(words):
        ax.text(reduced_vectors[i, 0], reduced_vectors[i, 1], reduced_vectors[i, 2], word, fontsize=10)

    ax.set_title(title)
    ax.set_xlabel("Component 1")
    ax.set_ylabel("Component 2")
    ax.set_zlabel("Component 3")
    plt.show()


# =============================================
# â–¶ï¸ Model gÃ¶rselleÅŸtirme Ã§aÄŸrÄ±larÄ±
plot_word_embedding(word2vec_model, "Word2Vec")
plot_word_embedding(fasttext_model, "FastText")

# =============================================
# ğŸ› ï¸ Spyder'da Grafiklerin Yeni Pencerede AÃ§Ä±lmasÄ± Ä°Ã§in Ayar:
"""
Spyder'da matplotlib grafiklerinin ayrÄ± pencere (figure) olarak aÃ§Ä±lmasÄ± iÃ§in:

1. Tools (AraÃ§lar) â†’ Preferences (Ayarlar)
2. IPython Console â†’ Graphics
3. Backend: [Automatic] yerine [Qt5 (new window)] seÃ§
4. Apply â†’ OK â†’ Spyder'Ä± yeniden baÅŸlat
"""
